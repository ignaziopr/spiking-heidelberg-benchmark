{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f4e778",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Comparative Analysis of Neural Network Architectures for Audio Classification on the Heidelberg Speech Dataset\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive benchmark study comparing multiple neural network architectures for audio classification tasks using the Heidelberg Speech Dataset (SHD). The implementation evaluates seven different models across four architectural paradigms: Spiking Neural Networks (SNNs), Spiking Recurrent Neural Networks (SRNNs), Long Short-Term Memory networks (LSTMs), and Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b83f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from heidelberg_dataset import HeidelbergDataset, HeidelbergDatasetLazy, HeidelbergDatasetBatchCached, HeidelbergDatasetCached\n",
    "from src.utils.save_model import save_model_checkpoint\n",
    "from models.architectures.snn import SpikingNeuralNetwork\n",
    "from models.architectures.lstm import LSTMClassifier\n",
    "from models.architectures.cnn import CNNClassifier\n",
    "from models.loss.max_over_time_loss import MaxOverTimeLoss\n",
    "from models.loss.standard_cross_entropy import LSTMCNNLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0478236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'train_file': '../data/shd_train.h5',\n",
    "    'test_file': '../data/shd_test.h5',\n",
    "    'batch_size': 128,\n",
    "    'num_epochs': 200,\n",
    "    'learning_rate': 1e-3,\n",
    "    'dt': 1e-3, \n",
    "    'T_max': 1.0, \n",
    "    'input_size': 700,\n",
    "    'hidden_size': 128,\n",
    "    'output_size': 20,\n",
    "    'dropout': 0.2,\n",
    "    'save_models': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45da8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_dataset_setup(model_type, CONFIG):\n",
    "    \"\"\"Train all models of a specific type, then clean up\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {model_type.upper()} datasets...\")\n",
    "    \n",
    "    # Load datasets for this model type only\n",
    "    train_dataset = HeidelbergDatasetCached(\n",
    "        CONFIG['train_file'], 'train', \n",
    "        dt=CONFIG['dt'], T_max=CONFIG['T_max'], \n",
    "        model_type=model_type\n",
    "    )\n",
    "    val_dataset = HeidelbergDatasetCached(\n",
    "        CONFIG['train_file'], 'validation', \n",
    "        dt=CONFIG['dt'], T_max=CONFIG['T_max'], \n",
    "        model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47228955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_dataset_clean(train_loader, val_loader):\n",
    "  del train_loader, val_loader\n",
    "  gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecf9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, is_snn=True, loss_type='max_over_time'):\n",
    "    \"\"\"Generic evaluation function\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Get predictions based on model type\n",
    "            if is_snn:\n",
    "                max_outputs, _ = torch.max(outputs, dim=1)\n",
    "                predictions = torch.argmax(max_outputs, dim=1)\n",
    "            elif isinstance(outputs, dict):  # LSTM\n",
    "                if loss_type == 'last_time_step':\n",
    "                    predictions = torch.argmax(outputs['last_time_step'], dim=1)\n",
    "                else:\n",
    "                    max_outputs, _ = torch.max(outputs['all_time_steps'], dim=1)\n",
    "                    predictions = torch.argmax(max_outputs, dim=1)\n",
    "            else:  # CNN\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return accuracy_score(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "740ff804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, model_name, num_epochs=150, lr=1e-3, \n",
    "                is_snn=True, loss_type='max_over_time'):\n",
    "    \"\"\"Generic training function for all models\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Training {model_name}...\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=lr)\n",
    "    \n",
    "    if is_snn:\n",
    "        criterion = MaxOverTimeLoss()\n",
    "    else:\n",
    "        criterion = LSTMCNNLoss(loss_type=loss_type)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"Training {model_name}\"):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            val_acc = evaluate_model(model, val_loader, is_snn=is_snn, loss_type=loss_type)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "    print(f\"✅ {model_name} training completed! Best Val Acc: {best_val_acc:.4f}\")\n",
    "    \n",
    "    final_val_acc = val_accuracies[-1]\n",
    "    \n",
    "    save_model_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=num_epochs-1,\n",
    "        loss=train_losses[-1],\n",
    "        accuracy=final_val_acc,\n",
    "        model_name=f\"{model_name}_final\",\n",
    "        checkpoint_dir=\"../models/checkpoints\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'optimizer': optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3601534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "✅ All models initialized!\n",
      "\n",
      "SNN_1Layer: 92,180 parameters\n",
      "\n",
      "SNN_2Layer: 108,564 parameters\n",
      "\n",
      "SNN_3Layer: 124,948 parameters\n",
      "\n",
      "SRNN: 108,564 parameters\n",
      "\n",
      "LSTM: 427,540 parameters\n",
      "\n",
      "CNN: 455,700 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioperez/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "# Spiking Neural Networks\n",
    "models['SNN_1Layer'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    num_layers=1,\n",
    "    recurrent=False,\n",
    "    dt=CONFIG['dt']\n",
    ")\n",
    "\n",
    "models['SNN_2Layer'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    num_layers=2,\n",
    "    recurrent=False,\n",
    "    dt=CONFIG['dt']\n",
    ")\n",
    "\n",
    "models['SNN_3Layer'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    num_layers=3,\n",
    "    recurrent=False,\n",
    "    dt=CONFIG['dt']\n",
    ")\n",
    "\n",
    "models['SRNN'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    dt=CONFIG['dt'],\n",
    "    recurrent=True\n",
    ")\n",
    "\n",
    "# LSTM\n",
    "models['LSTM'] = LSTMClassifier(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "\n",
    "# CNN\n",
    "models['CNN'] = CNNClassifier(\n",
    "    input_channels=64,  # Spatially binned channels for CNN\n",
    "    output_size=CONFIG['output_size'],\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "\n",
    "print(\"✅ All models initialized!\")\n",
    "# Print model summaries\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bcd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting training for all models...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training configuration for each model\n",
    "training_configs = {\n",
    "    'SNN_1Layer': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'SNN_2Layer': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'SNN_3Layer': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'SRNN': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'LSTM': {'model_type': 'lstm', 'is_snn': False, 'loss_type': 'max_over_time'},\n",
    "    'CNN': {'model_type': 'cnn', 'is_snn': False, 'loss_type': 'standard'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e07b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading SNN datasets...\n",
      "Loading train dataset into memory...\n",
      "✓ Loaded 8156 samples\n",
      "Loading validation dataset into memory...\n",
      "✓ Loaded 8156 samples\n",
      "Training SpikingNeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): SpikingLayer(\n",
      "      (lif): LIFNeuron()\n",
      "    )\n",
      "  )\n",
      "  (readout): Linear(in_features=128, out_features=20, bias=True)\n",
      ")...\n",
      "\n",
      "🚀 Training SNN_1Layer...\n",
      "Parameters: 92,180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf77d94b6a3548d4ad1e3f753a209ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training SNN_1Layer:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.9984, Val Acc = 0.0419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels[model_name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodels[model_name],\n\u001b[1;32m     15\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     16\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     17\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     18\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m     lr\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     20\u001b[0m     is_snn\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_snn\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     21\u001b[0m     loss_type\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     25\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training_time\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, model_name, num_epochs, lr, is_snn, loss_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_results = {}\n",
    "\n",
    "print(\"🎯 Starting training for all models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train each model\n",
    "for model_name in models.keys():\n",
    "    config = training_configs[model_name]\n",
    "    train_loader, val_loader = single_model_dataset_setup(\n",
    "        model_type=config['model_type'],\n",
    "        CONFIG=CONFIG\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Training {models[model_name]}...\")\n",
    "\n",
    "    result = train_model(\n",
    "        model=models[model_name],\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model_name=model_name,\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        is_snn=config['is_snn'],\n",
    "        loss_type=config['loss_type']\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    result['training_time'] = training_time\n",
    "    training_results[model_name] = result\n",
    "\n",
    "    single_model_dataset_clean(train_loader, val_loader)\n",
    "\n",
    "    print(f\"⏱️  {models[model_name]} Training time: {training_time/60:.2f} minutes\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"🎉 All models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, results) in enumerate(training_results.items()):\n",
    "\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot training loss\n",
    "    ax.plot(results['train_losses'], label='Training Loss', alpha=0.7)\n",
    "\n",
    "    # Plot validation accuracy (scaled for visualization)\n",
    "    val_epochs = list(range(0, len(results['train_losses']), 10))[\n",
    "        :len(results['val_accuracies'])]\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(val_epochs, results['val_accuracies'],\n",
    "             'r-', label='Validation Accuracy')\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Training Loss', color='b')\n",
    "    ax2.set_ylabel('Validation Accuracy', color='r')\n",
    "    ax.set_title(f'{model_name}\\nBest Val Acc: {results[\"best_val_acc\"]:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Training Progress for All Models', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
