{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f4e778",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Comparative Analysis of Neural Network Architectures for Audio Classification on the Heidelberg Speech Dataset\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive benchmark study comparing multiple neural network architectures for audio classification tasks using the Heidelberg Speech Dataset (SHD). The implementation evaluates seven different models across four architectural paradigms: Spiking Neural Networks (SNNs), Spiking Recurrent Neural Networks (SRNNs), Long Short-Term Memory networks (LSTMs), and Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b83f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from heidelberg_dataset import HeidelbergDatasetCached\n",
    "from src.utils.save_model import save_model_checkpoint\n",
    "from models.architectures.snn import SpikingNeuralNetwork\n",
    "from models.architectures.lstm import LSTMClassifier\n",
    "from models.architectures.cnn import CNNClassifier\n",
    "from models.loss.max_over_time_loss import MaxOverTimeLoss\n",
    "from models.loss.standard_cross_entropy import LSTMCNNLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0478236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'train_file': '../data/shd_train.h5',\n",
    "    'test_file': '../data/shd_test.h5',\n",
    "    'batch_size': 256,\n",
    "    'num_epochs': 300,\n",
    "    'learning_rate': 0.002,\n",
    "    'dt': 0.14e-3, \n",
    "    'T_max': 1.4, \n",
    "    'input_size': 700,\n",
    "    'hidden_size': 200,\n",
    "    'output_size': 20,\n",
    "    'nb_steps': 200,\n",
    "    'dropout': 0.1,\n",
    "    'save_models': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f92183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable-length sequences\"\"\"\n",
    "    data, labels = zip(*batch)\n",
    "    data = torch.stack(data, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45da8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_dataset_setup(model_type, CONFIG, val_split=0.12):\n",
    "    \"\"\"Load datasets with proper train/val split - UPDATED for paper compatibility\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {model_type.upper()} datasets...\")\n",
    "    \n",
    "    if model_type == 'snn':\n",
    "        nb_steps = CONFIG['nb_steps']\n",
    "    else:\n",
    "        nb_steps = int(CONFIG['T_max'] / 0.01)\n",
    "    \n",
    "    print(f\"Using {nb_steps} time steps for {model_type.upper()}\")\n",
    "    \n",
    "    # Load full training dataset\n",
    "    full_train_dataset = HeidelbergDatasetCached(\n",
    "        CONFIG['train_file'], 'train', \n",
    "        dt=CONFIG['dt'], T_max=CONFIG['T_max'], \n",
    "        model_type=model_type, nb_steps=nb_steps\n",
    "    )\n",
    "    \n",
    "    # Create train/val split\n",
    "    total_size = len(full_train_dataset)\n",
    "    val_size = int(total_size * val_split)\n",
    "    train_size = total_size - val_size\n",
    "    \n",
    "    # Random split\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {train_size}, Val: {val_size}\")\n",
    "    \n",
    "    # Create dataloaders with custom collate function\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=True, \n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=CONFIG['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47228955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_dataset_clean(train_loader, val_loader):\n",
    "  del train_loader, val_loader\n",
    "  gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecf9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, is_snn=True, loss_type='max_over_time'):\n",
    "    \"\"\"Generic evaluation function\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            if is_snn and isinstance(outputs, tuple):\n",
    "                outputs, _ = outputs  \n",
    "                max_outputs, _ = torch.max(outputs, dim=1)\n",
    "                predictions = torch.argmax(max_outputs, dim=1)\n",
    "            elif isinstance(outputs, dict):  # LSTM\n",
    "                if loss_type == 'last_time_step':\n",
    "                    predictions = torch.argmax(outputs['last_time_step'], dim=1)\n",
    "                else:\n",
    "                    max_outputs, _ = torch.max(outputs['all_time_steps'], dim=1)\n",
    "                    predictions = torch.argmax(max_outputs, dim=1)\n",
    "            else:  # CNN\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return accuracy_score(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740ff804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, model_name, num_epochs=150, lr=1e-3, \n",
    "                is_snn=True, loss_type='max_over_time'):\n",
    "    \"\"\"Generic training function for all models\"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸš€ Training {model_name}...\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=lr, betas=(0.9,0.999))\n",
    "    \n",
    "    if is_snn:\n",
    "        criterion = MaxOverTimeLoss()\n",
    "    else:\n",
    "        criterion = LSTMCNNLoss(loss_type=loss_type)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"Training {model_name}\"):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch % 5 == 0 and epoch != 0) or epoch == 1:\n",
    "            val_acc = evaluate_model(model, val_loader, is_snn=is_snn, loss_type=loss_type)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "    print(f\"âœ… {model_name} training completed! Best Val Acc: {best_val_acc:.4f}\")\n",
    "    \n",
    "    final_val_acc = val_accuracies[-1]\n",
    "    \n",
    "    save_model_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=num_epochs-1,\n",
    "        loss=train_losses[-1],\n",
    "        accuracy=final_val_acc,\n",
    "        model_name=f\"{model_name}_final\",\n",
    "        checkpoint_dir=\"../models/checkpoints\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'optimizer': optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3601534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "âœ… All models initialized!\n",
      "\n",
      "LSTM: 725,620 parameters\n",
      "\n",
      "CNN: 455,700 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioperez/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "\"\"\"\n",
    "# Spiking Neural Networks\n",
    "models['SNN_1Layer'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    num_layers=1,\n",
    "    recurrent=False,\n",
    "    dt=CONFIG['dt'],\n",
    "    reg_strength=2e-6\n",
    ")\n",
    "\n",
    "models['SNN_2Layer'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    num_layers=2,\n",
    "    recurrent=False,\n",
    "    dt=CONFIG['dt']\n",
    ")\n",
    "\n",
    "models['SNN_3Layer'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    num_layers=3,\n",
    "    recurrent=False,\n",
    "    dt=CONFIG['dt']\n",
    ")\n",
    "\n",
    "models['SRNN'] = SpikingNeuralNetwork(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    dt=CONFIG['dt'],\n",
    "    recurrent=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# LSTM\n",
    "models['LSTM'] = LSTMClassifier(\n",
    "    input_size=CONFIG['input_size'],\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    output_size=CONFIG['output_size'],\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "\n",
    "# CNN\n",
    "models['CNN'] = CNNClassifier(\n",
    "    input_channels=64,  # Spatially binned channels for CNN\n",
    "    output_size=CONFIG['output_size'],\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "\n",
    "print(\"âœ… All models initialized!\")\n",
    "# Print model summaries\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7bcd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for each model\n",
    "training_configs = {\n",
    "    'SNN_1Layer': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'SNN_2Layer': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'SNN_3Layer': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'SRNN': {'model_type': 'snn', 'is_snn': True, 'loss_type': 'max_over_time'},\n",
    "    'LSTM': {'model_type': 'lstm', 'is_snn': False, 'loss_type': 'max_over_time'},\n",
    "    'CNN': {'model_type': 'cnn', 'is_snn': False, 'loss_type': 'standard'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e07b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Starting training for all models...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Loading LSTM datasets...\n",
      "Using 140 time steps for LSTM\n",
      "Loading train dataset into memory...\n",
      "âœ“ Loaded 8156 samples\n",
      "âœ“ Time bins: 140 steps over 1.4s\n",
      "Dataset sizes - Train: 7178, Val: 978\n",
      "Training LSTMClassifier(\n",
      "  (lstm): LSTM(700, 200, batch_first=True, dropout=0.1)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=200, out_features=20, bias=True)\n",
      ")...\n",
      "\n",
      "ðŸš€ Training LSTM...\n",
      "Parameters: 725,620\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0c49ef69fe42e99078ed792dea3a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training LSTM:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_results = {}\n",
    "\n",
    "print(\"ðŸŽ¯ Starting training for all models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train each model\n",
    "for model_name in models.keys():\n",
    "    config = training_configs[model_name]\n",
    "    train_loader, val_loader = single_model_dataset_setup(\n",
    "        model_type=config['model_type'],\n",
    "        CONFIG=CONFIG\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Training {models[model_name]}...\")\n",
    "\n",
    "    result = train_model(\n",
    "        model=models[model_name],\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model_name=model_name,\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        is_snn=config['is_snn'],\n",
    "        loss_type=config['loss_type']\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    result['training_time'] = training_time\n",
    "    training_results[model_name] = result\n",
    "\n",
    "    single_model_dataset_clean(train_loader, val_loader)\n",
    "\n",
    "    print(f\"â±ï¸  {models[model_name]} Training time: {training_time/60:.2f} minutes\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(\"ðŸŽ‰ All models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, results) in enumerate(training_results.items()):\n",
    "\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot training loss\n",
    "    ax.plot(results['train_losses'], label='Training Loss', alpha=0.7)\n",
    "\n",
    "    # Plot validation accuracy (scaled for visualization)\n",
    "    val_epochs = list(range(0, len(results['train_losses']), 10))[\n",
    "        :len(results['val_accuracies'])]\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(val_epochs, results['val_accuracies'],\n",
    "             'r-', label='Validation Accuracy')\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Training Loss', color='b')\n",
    "    ax2.set_ylabel('Validation Accuracy', color='r')\n",
    "    ax.set_title(f'{model_name}\\nBest Val Acc: {results[\"best_val_acc\"]:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Training Progress for All Models', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
